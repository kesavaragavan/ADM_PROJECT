{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0c3772cefced327d389bc32698ee9f60a8ec467f752250ff9d98d1c4661a7242b",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "c3772cefced327d389bc32698ee9f60a8ec467f752250ff9d98d1c4661a7242b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from deap import creator, base, tools, algorithms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['## importing packages.py', 'city_day.csv', 'city_hour.csv', 'Dataset', 'Final', 'FUZZY', 'GA', 'GA.ipynb', 'GA1.ipynb', 'Ruleset', 'RuleSet_Creation (2).ipynb', 'stations.csv', 'station_day.csv', 'station_hour.csv', 't2.arff', 'test1.csv', 'test1.py', 'test2.csv', 'test2.py', 'test3.py', 'test4.py', 'test5f.py', 'test6f.py', 'test7GA.py', 'Test8.py']\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 29531 entries, 0 to 29530\nData columns (total 16 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   City        29531 non-null  object \n 1   Date        29531 non-null  object \n 2   PM2.5       24933 non-null  float64\n 3   PM10        18391 non-null  float64\n 4   NO          25949 non-null  float64\n 5   NO2         25946 non-null  float64\n 6   NOx         25346 non-null  float64\n 7   NH3         19203 non-null  float64\n 8   CO          27472 non-null  float64\n 9   SO2         25677 non-null  float64\n 10  O3          25509 non-null  float64\n 11  Benzene     23908 non-null  float64\n 12  Toluene     21490 non-null  float64\n 13  Xylene      11422 non-null  float64\n 14  AQI         24850 non-null  float64\n 15  AQI_Bucket  24850 non-null  object \ndtypes: float64(13), object(3)\nmemory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#loading dataset \n",
    "\n",
    "print(os.listdir(\"../archive (1)\"))\n",
    "\n",
    "# Be careful with path Directory.\n",
    "#reading dataset\n",
    "\n",
    "dataset=pd.read_csv('../archive (1)/city_day.csv')\n",
    "dataset.describe()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Each labels and total number of tuples accordingly\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.0    16618\n",
       "2.0     6435\n",
       "1.0     3169\n",
       "4.0     2151\n",
       "5.0      766\n",
       "6.0      392\n",
       "Name: AQI_bucket_calculated, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "#removing null\n",
    "#print(aha,\"------------>\",len(aha))\n",
    "dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(dataset.iloc[:,2:14].values)\n",
    "dataset.iloc[:,2:14] = imputer.transform(dataset.iloc[:,2:14].values)\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "def AIR_QUALITY_INDEX(x):\n",
    "    if x <= 50:\n",
    "        return 1.0\n",
    "    elif x <= 100:\n",
    "        return 2.0\n",
    "    elif x <= 200:\n",
    "        return 3.0\n",
    "    elif x <= 300:\n",
    "        return 4.0\n",
    "    elif x <= 400:\n",
    "        return 5.0\n",
    "    elif x > 400:\n",
    "        return 6.0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "dataset[\"Checks\"] = (dataset[\"PM2.5\"] > 0).astype(int) + \\\n",
    "                (dataset[\"PM10\"] > 0).astype(int) + \\\n",
    "                (dataset[\"SO2\"] > 0).astype(int) + \\\n",
    "                (dataset[\"NOx\"] > 0).astype(int) + \\\n",
    "                (dataset[\"NH3\"] > 0).astype(int) + \\\n",
    "                (dataset[\"CO\"] > 0).astype(int) + \\\n",
    "                (dataset[\"O3\"] > 0).astype(int)\n",
    "\n",
    "dataset[\"AQI_calculated\"] = round(dataset[[\"PM2.5\", \"PM10\", \"SO2\", \"NOx\",\n",
    "                                 \"NH3\", \"CO\", \"O3\"]].max(axis = 1))\n",
    "dataset.loc[\"dag\"] = np.NaN\n",
    "dataset.loc[dataset.Checks < 3, \"AQI_calculated\"] = np.NaN\n",
    "\n",
    "dataset[\"AQI_bucket_calculated\"] = dataset[\"AQI_calculated\"].apply(lambda x: AIR_QUALITY_INDEX(x))\n",
    "dataset[~dataset.AQI_calculated.isna()].head(13)\n",
    "print(\"Each labels and total number of tuples accordingly\")\n",
    "dataset[~dataset.AQI_calculated.isna()].AQI_bucket_calculated.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Ahmedabad' 'Aizawl' 'Amaravati' 'Amritsar' 'Bengaluru' 'Bhopal'\n 'Brajrajnagar' 'Chandigarh' 'Chennai' 'Coimbatore' 'Delhi' 'Ernakulam'\n 'Gurugram' 'Guwahati' 'Hyderabad' 'Jaipur' 'Jorapokhar' 'Kochi' 'Kolkata'\n 'Lucknow' 'Mumbai' 'Patna' 'Shillong' 'Talcher' 'Thiruvananthapuram'\n 'Visakhapatnam' nan]\n<class 'pandas.core.frame.DataFrame'>\nIndex: 2006 entries, 14581 to 16586\nData columns (total 19 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   City                   2006 non-null   object \n 1   Date                   2006 non-null   object \n 2   PM2.5                  2006 non-null   float64\n 3   PM10                   2006 non-null   float64\n 4   NO                     2006 non-null   float64\n 5   NO2                    2006 non-null   float64\n 6   NOx                    2006 non-null   float64\n 7   NH3                    2006 non-null   float64\n 8   CO                     2006 non-null   float64\n 9   SO2                    2006 non-null   float64\n 10  O3                     2006 non-null   float64\n 11  Benzene                2006 non-null   float64\n 12  Toluene                2006 non-null   float64\n 13  Xylene                 2006 non-null   float64\n 14  AQI                    1880 non-null   float64\n 15  AQI_Bucket             1880 non-null   object \n 16  Checks                 2006 non-null   float64\n 17  AQI_calculated         2006 non-null   float64\n 18  AQI_bucket_calculated  2006 non-null   float64\ndtypes: float64(16), object(3)\nmemory usage: 313.4+ KB\n"
     ]
    }
   ],
   "source": [
    "print(dataset['City'].unique())\n",
    "df = dataset\n",
    "dataset = dataset[dataset['City'] == 'Hyderabad']\n",
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg(l):\n",
    "    #this returns average among list element\n",
    "    return (sum(l)/float(len(l)))\n",
    "\n",
    "\n",
    "def getFitness(Ind, X, y):\n",
    "    #returns the subset of features fitness\n",
    "    if(Ind.count(0) != len(Ind)):\n",
    "        # get i with value 0\n",
    "        Column = [i for i in range(\n",
    "            len(Ind)) if Ind[i] == 0]\n",
    "\n",
    "        # get features subset\n",
    "        parse = X.drop(X.columns[Column], axis=1)\n",
    "        sub_features = pd.get_dummies(parse)\n",
    "\n",
    "        # Classifier model is loaded \n",
    "\n",
    "        #clf = LogisticRegression()\n",
    "        #clf = DecisionTreeClassifier(max_depth=10)\n",
    "        clf = RandomForestRegressor()\n",
    "        return (avg(cross_val_score(clf, sub_features, y, cv=5)),)\n",
    "    else:\n",
    "        return(0,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GA(X, y, n_population, n_generation):\n",
    "    # Deap global variables are intiated\n",
    "    # creating Ind\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Ind\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    # create toolbox and tournament is set\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    toolbox.register(\"Ind\", tools.initRepeat,\n",
    "                     creator.Ind, toolbox.attr_bool, len(X.columns))\n",
    "    toolbox.register(\"population\", tools.initRepeat, list,\n",
    "                     toolbox.Ind)\n",
    "    toolbox.register(\"evaluate\", getFitness, X=X, y=y)\n",
    "    toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # initialize parameters\n",
    "    pop = toolbox.population(n=n_population)\n",
    "    hof = tools.HallOfFame(n_population * n_generation)\n",
    "    stat_oper = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stat_oper.register(\"avg\", np.mean)\n",
    "    stat_oper.register(\"min\", np.min)\n",
    "    stat_oper.register(\"max\", np.max)\n",
    "\n",
    "    # genetic algorithm \n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n",
    "                                   ngen=n_generation, stats=stat_oper, halloffame=hof,\n",
    "                                   verbose=True)\n",
    "\n",
    "    # return hall of fame\n",
    "    return hof\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bestInd(hof, X, y):\n",
    "    #selecting best Individuals\n",
    "    Max_Acc = 0.0\n",
    "    for Ind in hof:\n",
    "        #print(type(Ind.fitness.values[0]),Ind.fitness.values[0],\"-.-.-.\")\n",
    "        if(Ind.fitness.values[0] > Max_Acc):\n",
    "            Max_Acc = Ind.fitness.values\n",
    "            _Ind = Ind\n",
    "\n",
    "    _IndHeader = [list(X)[i] for i in range(\n",
    "        len(_Ind)) if _Ind[i] == 1]\n",
    "    return _Ind.fitness.values, _Ind, _IndHeader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dir, n_pop, n_gen = getArguments()\n",
    "Dir = \"../archive (1)/city_day.csv\"\n",
    "n_pop = 3  # population\n",
    "n_gen = 10 #generation\n",
    "y = dataset.iloc[:, -1]\n",
    "X = dataset.iloc[:,2:14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Calucalted for all features: \t(0.9440136902911025,)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# all features accuracy is calculated for base\n",
    "individual = [1 for i in range(len(X.columns))]\n",
    "print(\"Accuracy Calucalted for all features: \\t\" +str(getFitness(individual, X, y)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gen\tnevals\tavg     \tmin     \tmax     \n",
      "0  \t3     \t0.485254\t0.202849\t0.813292\n",
      "1  \t2     \t0.811698\t0.810485\t0.813292\n",
      "2  \t0     \t0.812634\t0.811319\t0.813292\n",
      "3  \t3     \t0.801692\t0.792427\t0.816545\n",
      "4  \t2     \t0.85362 \t0.797064\t0.947251\n",
      "5  \t1     \t0.946923\t0.946267\t0.947251\n",
      "6  \t1     \t0.947184\t0.94705 \t0.947251\n",
      "7  \t1     \t0.947352\t0.947251\t0.947553\n",
      "8  \t0     \t0.947452\t0.947251\t0.947553\n",
      "9  \t3     \t0.727538\t0.293333\t0.945052\n",
      "10 \t0     \t0.945052\t0.945052\t0.945052\n",
      "Best Accuracy: \t(0.9472513618140408,)\n",
      "Number of Features in Subset: \t4\n",
      "Individual: \t\t[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "Feature Subset\t: ['PM2.5', 'PM10', 'NOx', 'Toluene']\n"
     ]
    }
   ],
   "source": [
    "#  genetic algorithm\n",
    "hof = GA(X, y, n_pop, n_gen)\n",
    "# select the best individual\n",
    "accuracy, individual, header = bestInd(hof, X, y)\n",
    "print('Best Accuracy: \\t' + str(accuracy))\n",
    "print('Number of Features in Subset: \\t' + str(individual.count(1)))\n",
    "print('Individual: \\t\\t' + str(individual))\n",
    "print('Feature Subset\\t: ' + str(header))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "#calculating accuracy for selected final sub set features\n",
    "#df = pd.read_csv(Dir, sep=',')\n",
    "\n",
    "# with feature subset\n",
    "\n",
    "X = X.loc[:,header]\n",
    "y = dataset.iloc[:, -1]\n",
    "#genetic algorithm works with RandomForestRegressor were as final clf used here is DecisionTreeClassifier\n",
    "clf =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)\n",
    "Accuracy = cross_val_score(clf, X, y, cv=cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overall Accuracy with Feature Subset: \t0.9893310945273636\n\n_________________________________\nOverall Optimal Feature selection Result:\n_________________________________\nPM2.5\nPM10\nNOx\nToluene\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy with Feature Subset: \\t\" + str(avg(Accuracy)) + \"\\n\")\n",
    "print(\"_________________________________\")\n",
    "print(\"Overall Optimal Feature selection Result:\")\n",
    "print(\"_________________________________\")\n",
    "for i in range(0,len(header)):\n",
    "    print(header[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------> 0.995023631840796\n",
      "2009 2009\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'Ind' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "gen\tnevals\tavg     \tmin     \tmax     \n",
      "0  \t3     \t-13.9357\t-29.8062\t0.401397\n",
      "1  \t2     \t-3.86871\t-12.4024\t0.407425\n",
      "2  \t2     \t0.401052\t0.388867\t0.407316\n",
      "3  \t0     \t0.407316\t0.407316\t0.407316\n",
      "4  \t1     \t0.357059\t0.256544\t0.407316\n",
      "5  \t2     \t0.406731\t0.404015\t0.408861\n",
      "6  \t1     \t0.401939\t0.389642\t0.408861\n",
      "7  \t2     \t0.392029\t0.381692\t0.407316\n",
      "8  \t0     \t0.400571\t0.38708 \t0.407316\n",
      "9  \t3     \t0.347001\t0.270686\t0.397648\n",
      "10 \t0     \t0.380996\t0.372669\t0.397648\n",
      "2009 2009\n",
      "2009 2009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Chennai specific attribute selection instead of city\n",
    "l = df[df[\"City\"] == 'Chennai']\n",
    "\n",
    "y = l.iloc[:,-1]\n",
    "X = l.iloc[:,2:14]\n",
    "clfp =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n",
    "Accuracy_Chennai = cross_val_score(clfp, X, y, cv=cv)\n",
    "print(\"--------->\",avg(Accuracy_Chennai))\n",
    "\n",
    "print(len(X),len(y))\n",
    "\n",
    "#  genetic algorithm\n",
    "hof = GA(X, y, n_pop, n_gen)\n",
    "# select the best individual\n",
    "accuracy_Chennai, individual_Chennai, header_Chennai = bestInd(hof, X, y)\n",
    "\n",
    "print(len(X),len(y))\n",
    "X = X.loc[:,header_Chennai]\n",
    "#y = l.iloc[:, :-1]\n",
    "\n",
    "print(len(X),len(y))\n",
    "#genetic algorithm works with RandomForestRegressor were as final clf used here is DecisionTreeClassifier\n",
    "clff =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n",
    "Accuracy_Chennai = cross_val_score(clff, X, y, cv=cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chennai Accuracy with Feature Subset: \t0.9721218905472634\n\n_________________________________\nChennai Optimal Feature selection Result:\n_________________________________\nPM10\nNH3\nXylene\n"
     ]
    }
   ],
   "source": [
    "print(\"Chennai Accuracy with Feature Subset: \\t\" + str(avg(Accuracy_Chennai)) + \"\\n\")\n",
    "print(\"_________________________________\")\n",
    "print(\"Chennai Optimal Feature selection Result:\")\n",
    "print(\"_________________________________\")\n",
    "for i in range(0,len(header_Chennai)):\n",
    "    print(header_Chennai[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'Ind' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "---------> 0.9935286069651742\n",
      "2009 2009\n",
      "gen\tnevals\tavg      \tmin     \tmax     \n",
      "0  \t3     \t-0.392502\t-1.56117\t0.691934\n",
      "1  \t3     \t0.25328  \t-0.615169\t0.689359\n",
      "2  \t3     \t0.690006 \t0.686733 \t0.695187\n",
      "3  \t2     \t0.697416 \t0.695187 \t0.700358\n",
      "4  \t0     \t0.697922 \t0.696704 \t0.700358\n",
      "5  \t1     \t0.700869 \t0.700358 \t0.701891\n",
      "6  \t1     \t0.700308 \t0.697144 \t0.701891\n",
      "7  \t2     \t0.699445 \t0.696223 \t0.701891\n",
      "8  \t2     \t0.697894 \t0.690931 \t0.701891\n",
      "9  \t0     \t0.697894 \t0.690931 \t0.701891\n",
      "10 \t0     \t0.701547 \t0.700861 \t0.701891\n",
      "2009 2009\n",
      "2009 2009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Ahmedabad specific attribute selection instead of city\n",
    "l = df[df[\"City\"] == 'Ahmedabad']\n",
    "\n",
    "y = l.iloc[:,-1]\n",
    "X = l.iloc[:,2:14]\n",
    "clfp =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n",
    "Accuracy_Ahmedabad = cross_val_score(clfp, X, y, cv=cv)\n",
    "print(\"--------->\",avg(Accuracy_Ahmedabad))\n",
    "\n",
    "\n",
    "print(len(X),len(y))\n",
    "\n",
    "#  genetic algorithm\n",
    "hof = GA(X, y, n_pop, n_gen)\n",
    "# select the best individual\n",
    "accuracy_Ahmedabad, individual_Ahmedabad, header_Ahmedabad = bestInd(hof, X, y)\n",
    "\n",
    "print(len(X),len(y))\n",
    "X = X.loc[:,header_Ahmedabad]\n",
    "#y = l.iloc[:, :-1]\n",
    "\n",
    "print(len(X),len(y))\n",
    "#genetic algorithm works with RandomForestRegressor were as final clf used here is DecisionTreeClassifier\n",
    "clff =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ahmedabad Accuracy with Feature Subset: \t0.9947748756218905\n",
      "\n",
      "_________________________________\n",
      "Ahmedabad Optimal Feature selection Result:\n",
      "_________________________________\n",
      "PM2.5\n",
      "PM10\n",
      "NO\n",
      "NO2\n",
      "NH3\n",
      "CO\n",
      "SO2\n",
      "O3\n",
      "Benzene\n",
      "Toluene\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "Accuracy_Ahmedabad = cross_val_score(clff, X, y, cv=cv)\n",
    "print(\"Ahmedabad Accuracy with Feature Subset: \\t\" + str(avg(Accuracy_Ahmedabad)) + \"\\n\")\n",
    "print(\"_________________________________\")\n",
    "print(\"Ahmedabad Optimal Feature selection Result:\")\n",
    "print(\"_________________________________\")\n",
    "for i in range(0,len(header_Ahmedabad)):\n",
    "    print(header_Ahmedabad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------> 0.9945236318407961\n",
      "2009 2009\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\HM-PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\deap\\creator.py:138: RuntimeWarning: A class named 'Ind' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "gen\tnevals\tavg     \tmin     \tmax    \n",
      "0  \t3     \t0.763735\t0.346699\t0.97581\n",
      "1  \t0     \t0.973439\t0.968696\t0.97581\n",
      "2  \t1     \t0.975911\t0.97581 \t0.976112\n",
      "3  \t0     \t0.976112\t0.976112\t0.976112\n",
      "4  \t2     \t0.976032\t0.975887\t0.976112\n",
      "5  \t0     \t0.976112\t0.976112\t0.976112\n",
      "6  \t3     \t0.976283\t0.975938\t0.976734\n",
      "7  \t3     \t0.976623\t0.97647 \t0.97687 \n",
      "8  \t0     \t0.976756\t0.976527\t0.97687 \n",
      "9  \t3     \t0.976806\t0.97667 \t0.976922\n",
      "10 \t1     \t0.974763\t0.970444\t0.976922\n",
      "2009 2009\n",
      "2009 2009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Delhi specific attribute selection instead of city\n",
    "l = df[df[\"City\"] == 'Delhi']\n",
    "\n",
    "y = l.iloc[:,-1]\n",
    "X = l.iloc[:,2:14]\n",
    "clfp =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n",
    "Accuracy_Delhi = cross_val_score(clfp, X, y, cv=cv)\n",
    "print(\"--------->\",avg(Accuracy_Delhi))\n",
    "print(len(X),len(y))\n",
    "\n",
    "#  genetic algorithm\n",
    "hof = GA(X, y, n_pop, n_gen)\n",
    "# select the best individual\n",
    "accuracy_Delhi, individual_Delhi, header_Delhi = bestInd(hof, X, y)\n",
    "\n",
    "print(len(X),len(y))\n",
    "X = X.loc[:,header_Delhi]\n",
    "#y = l.iloc[:, :-1]\n",
    "\n",
    "print(len(X),len(y))\n",
    "#genetic algorithm works with RandomForestRegressor were as final clf used here is DecisionTreeClassifier\n",
    "clff =DecisionTreeClassifier(max_depth=5,random_state=1)\n",
    "#cross validating\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Delhi Accuracy with Feature Subset: \t0.9940248756218905\n\n_________________________________\nDelhi Optimal Feature selection Result:\n_________________________________\nPM2.5\nPM10\nNO\nNO2\nNOx\nO3\n"
     ]
    }
   ],
   "source": [
    "Accuracy_Delhi = cross_val_score(clff, X, y, cv=cv)\n",
    "print(\"Delhi Accuracy with Feature Subset: \\t\" + str(avg(Accuracy_Delhi)) + \"\\n\")\n",
    "print(\"_________________________________\")\n",
    "print(\"Delhi Optimal Feature selection Result:\")\n",
    "print(\"_________________________________\")\n",
    "for i in range(0,len(header_Delhi)):\n",
    "    print(header_Delhi[i])"
   ]
  }
 ]
}